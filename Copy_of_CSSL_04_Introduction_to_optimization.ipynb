{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelleabraham-git/Stats_minor/blob/main/Copy_of_CSSL_04_Introduction_to_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is *optimization*?\n",
        "Optimization means finding the best choice among many possibilities. We encounter it in many aspects of our daily lives, such as: when we are tuning our recipe indegredients to make the food taste better, when we finding the best route to take to get to a place quickly.\n",
        "\n",
        "Much of statistical learning deals with building models to make predictions. Optimization plays an important role here as well. In statistical learning, we optimize model settings so predictions are as close as possible to reality."
      ],
      "metadata": {
        "id": "ddXGKwVWt3S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us take a simple example. Suppose we are predicting students' test scores based on the hours spent studying. If we assume that more study hours (h) implies better test scores (s), we can begin with a linear simple model:\n",
        "\n",
        "$s=a+bh$\n",
        "\n",
        "This is the equation of a straight line. Optimization in this context means adjusting a and b so the line fits the data best.\n",
        "\n",
        "_But how do we know that a particular line fits the data best?_\n",
        "\n",
        "Short answer - loss functions! Loss functions measure how bad our predictions are.\n",
        "\n",
        "Good predictions->small loss\n",
        "\n",
        "bad predictions->large loss.\n",
        "\n",
        "So if want the model that _best_ fits the data, we need to minimize the loss."
      ],
      "metadata": {
        "id": "XZMOBgTr2AHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us take a simple loss function and understand how to minimize the loss.\n",
        "\n",
        "$loss = x^2$"
      ],
      "metadata": {
        "id": "Rm5z7sgKki2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYg2FBwGkT4P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function called loss_func that takes in as input some value and returns its square\n"
      ],
      "metadata": {
        "id": "78ev1ZHvkebm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us plot this curve to visualize the loss function"
      ],
      "metadata": {
        "id": "nIHpyY15k611"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 arrays of x_values and the corresponding y_values and plot the curve"
      ],
      "metadata": {
        "id": "yCvaO8Tfk0KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This curve has a single minimum value which occurs at x=0. Such functions which have a single global minimum are referred to as convex functions. When multiple minima exist, things get more difficult!"
      ],
      "metadata": {
        "id": "5PLWdI05lxrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try walking down this gradient. The strategy is pretty simple.\n",
        "1. Start with a guess\n",
        "2. Look at slope (aka gradient) at that point\n",
        "3. Take a small step in the downhill direction\n",
        "4. Repeat steps 1-3 until you reach the base of the valley."
      ],
      "metadata": {
        "id": "tf3CVrkCp26D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting point\n",
        "\n",
        "\n",
        "# How big each step is\n",
        "\n",
        "\n",
        "# Store steps for plotting\n",
        "\n",
        "# Create a loop for the repetition step and move downhill in each step\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VxqzffwSl05Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try visualizing the walk"
      ],
      "metadata": {
        "id": "xTbIo26jquOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the curve and the steps that you took to reach the minimum\n"
      ],
      "metadata": {
        "id": "84EbfL-jlBJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "1. Draw a curve with multiple minima. Why do you think it would be difficult to get to the absolute minimum (aka global minimum) in this case?\n",
        "2. Can you write a simple mathematical equation that has multiple minima?"
      ],
      "metadata": {
        "id": "viT3l8Tiut5D"
      }
    }
  ]
}